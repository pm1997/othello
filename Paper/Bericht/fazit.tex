\chapter{Fazit}
\label{Fazit}

\section{Bewertung der Agenten}
In diesem Abschnitt werden die Agenten \mxZitat{Monte Carlo} und \mxZitat{Alpha-Beta Pruning} verglichen. \todo{Erweitern}
\subsection{Der Agent \mxZitat{Monte Carlo}}
Zunächst wird auf die Leistung des Agenten \mxZitat{Monte Carlo} gegen den zufällig agierenden Agenten eingegangen. Dabei finden auch einige noch offene Fragen Erwähnung. Den Abschluss dieses Abschnitts bildet schließlich die Beschreibung weiterer Anpassungsmöglichkeiten des Agenten.  
\paragraph{Leistung gegen den Agenten \mxZitat{Random}}
Wie in Tabelle \ref{tbl:cmp-results} anhand der Vergleiche 1 bzw. 6 deutlich wird, hat der \mxZitat{Monte Carlo}-Agent sämtliche der insgesamt 2000 Testspielen gegen den \mxZitat{Random}-Agenten gewonnen. Dabei ist bis auf eine geringe Abweichung der durchschnittlich benötigten Rechenzeit pro Spiel unerheblich ob der Agent als Spieler 1 oder Spieler 2 auftritt. Damit ist der mit dem Spiel des Agenten verbundene Rechenaufwand in so fern berechtigt, dass er bessere Ergebnisse liefert als ein Agent der zufällige Züge durchführt.
\\Im Vergleich mit menschlichen Spielern ist jedoch davon auszugehen, dass diese keine zufälligen Züge ausführen. Stattdessen ist, ein entsprechendes Spielniveau vorausgesetzt, damit zu rechnen, dass ein Spieler Kenntnisse über das Spiel und außerdem durch Erfahrung verfeinerte Strategien einbringt. Dieser Vergleich konnte im Rahmen der Arbeit leider nicht durchgeführt werden und ist damit eine Fragestellung für weitere Untersuchungen.  
\paragraph{Weitere Variationsmöglichkeiten}
In den zur Bestimmung der für den Agenten verwendeten Parameter konnte beobachtet werden, dass die Gewinnwahrscheinlichkeit des Agenten von der Anzahl der mittels der Monte-Carlo Methode simulierten Spiele abhängt. In den betrachteten Fällen konnte durch die Erhöhung der Anzahl simulierter Spiele, also die Erhöhung des Parameters \code{big\_n}, im Allgemeinen eine Verbesserung der Gewinnwahrscheinlichkeit des Agenten erzielt werden. Gleichzeitig hat \cite{nijssen_2007} gezeigt, dass sich dieser Effekt mit steigender Größe von \code{big\_n} abschwächt. Zusätzlich dazu steigt mit der Anzahl der durchgeführten Simulationen auch die benötigte Rechenzeit. Zu Untersuchen wäre daher ob das überschreiten der Rechenzeit von fünf Minuten auf dem Referenzgerät eine weitere Verbesserung bringt und damit den erhöhten Aufwand rechtfertigt.

\subsection{Der Agent \mxZitat{Alpha-Beta Pruning}}
In diesem Abschnitt findet sich, ähnlich dem vorherigen, zunächst ein Vergleich des \mxZitat{Alpha-Beta-Pruning} Agenten mit dem zufällig agierenden Agenten. Im Anschluss werden weitere Variationsmöglichkeiten besprochen. 
\paragraph{Leistung im Vergleich zum Agenten \mxZitat{Ramdom}}
Im Vergleich mit dem zufällig spielenden Agenten gewann der Agent \mxZitat{Alpha-Beta Pruning} in allen Fällen bis auf den Vgl. 2 mehr Spiele. Deutlich wird auch, dass der Erfolg des Agenten im wesentlichen von der Strategie zur Bewertung der Zustände in der maximalen Suchtiefe abhängt. Auf die dabei verwendeten einzelnen Heuristiken, wird im Abschnitt \ref{sect:Fazit:Heuristiken} genauer eingegangen.
\\Kommt statt einer Heuristik in den Blättern des Baumes jedoch Monte-Carlo zum Einsatz, so werden nochmals deutlich bessere Ergebnisse erzielt. Dafür dauert die Berechnung eines einzelnen Zuges nochmals wesentlich länger. Damit ist die Kombination der Alpha-Beta Abschneiden Methode und der Monte-Carlo Methode als ein, zwar eher Rechenintensiver, jedoch durchaus erfolgreicher Ansatz zu betrachten. Besonders hervorzuheben ist dabei, dass zur Umsetzung kein Spezialwissen für Othello benötigt wird. Zu Untersuchen wäre in wie weit dieser Aspekt auf andere Spiele übertragbar ist.
\\An dieser Stelle ist damit festzuhalten, dass auch der \mxZitat{Alpha Beta Pruning}-Agent, die Wahl einer guten Heuristik vorausgesetzt, den Rechenaufwand in so fern rechtfertigt, das Spiel des Agenten meist besser ist als das des zufällig agierenden. Wie oben wäre auch hier die Gewinnwahrscheinlichkeit gegen menschliche Spieler Gegenstand weiterer Untersuchungen.
\\Auffällig ist außerdem, dass im Vergleich zu dem \mxZitat{Monte Carlo}-Agenten im Spiel mit dem \mxZitat{Random}-Agenten eine deutlich längere Rechenzeit benötigt wird um zu einem insgesamt schlechteren Ergebnis zu kommen. So gelang es dem Agenten \mxZitat{Monte Carlo} alle Spiele gegen den Agenten \mxZitat{Random} zu gewinnen. Der hier besprochene Agent gewinnt jedoch, selbst in seiner besten Variation, nur $83\%$ der Spiele. Auf die Performance der beiden Agenten im direkten Vergleich wird in Abschnitt \ref{subsec:Fazit:AgentenVgl} eingegangen.
\paragraph{Weitere Variationsmöglichkeiten}
Gemäß der in Kapitel \ref{ab-pruning} beschriebenen Theorie hinter dem Agenten \mxZitat{Alpha-Beta Pruning} kann durch eine größere Suchtiefe \code{search\_depth} die Genauigkeit mit der die Bewertung eines Zuges der realitätsgetreuen Bewertung entspricht erhöht werden. Da sich der Suchbaum mit jeder Ebene stärker verzweigt ist in diesem Fall jedoch mit einer exponentiell steigenden Rechenzeit zu rechnen. Mit der vorgestellten Implementierung wäre dies unter dem Zeitaspekt jedoch keine Option. Entsprechend stellt sich hier die Frage in wie weit der Algorithmus des Alpha Beta Abschneidens parallelisiert werden kann um alle verfügbaren Prozessoren eines Computersystems zu benutzen und damit innerhalb der fünf Minuten Rechenzeit die der Anwender bereit ist zu warten, den Suchbaum noch tiefer zu durchsuchen.


\subsection{Vergleich der Agenten \mxZitat{Monte Carlo} und \mxZitat{Alpha-Beta Pruning}}
\label{subsec:Fazit:AgentenVgl}
Der \mxZitat{Monte Carlo} Agent spielt deutlich besser als der \mxZitat{Random} Agent. Der \mxZitat{Alpha-Beta Pruning} Agent ist bei der Verwendung der \mxZitat{Cowthello} Heuristik ebenfalls deutlich besser als der \mxZitat{Random} Agent, aber dennoch schlechter als der \mxZitat{Monte Carlo} Agent bei ähnlicher Spieldauer.
\\Da der \mxZitat{Monte Carlo} Agent in der Spieltheorie einfach nur aus einer gewissen Anzahl an zufälligen Spielen ab der aktuellen Spielposition den besten Folgezug zurückgibt, besitzt dieser Agent keinerlei Wissen über das Spielprinzip des Spiels Othello. 
\\Dies sollte durch die Verwendung des \mxZitat{Alpha-Beta Pruning} Agenten verbessert werden. Dieser basiert grundlegend auf einer angepassten MiniMax-Suche. Diese deckt den Aspekt des Spiels ab, dass gute Züge des eines Spielers gleichzeitig schlechte Züge des Gegners sind.
\\Der erwartete Effekt trat allerdings nicht ein. Dies kann mehrere Gründe haben:
\begin{itemize}
\item Die verfügbaren Heuristiken, welche in den Blattknoten der \mxZitat{Alpha-Beta Pruning} Suche verwendet werden, sind zu ungenau. Beispielsweise wurde in Kapitel \ref{fz:h_smc} festgestellt, dass die Heuristik \mxZitat{Stored Monte-Carlo} ungeeignet ist. Durch die große Spanne der Gewinnwahrscheinlichkeiten des \mxZitat{Alpha-Beta Pruning} Agenten je nach verwendeter Heuristik ist es denkbar, dass noch bessere Heuristiken existieren.
\item Die \mxZitat{Alpha-Beta Pruning} Suche benötigt sehr viel Zeit. Durch die große Verzweigung existieren sehr viele Blattknoten, für welche ein Wert ermittelt werden muss. Dies geschieht durch die Verwendung einer Heuristik oder der \mxZitat{Monte Carlo} Suche.
\\In der Zeit, welche der \mxZitat{Alpha-Beta Pruning} Agent zur Suche benötigt, kann der \mxZitat{Monte Carlo} Agent deutlich mehr zufällige Spiele berechnen. Dadurch wird der Vorteil des \mxZitat{Alpha-Beta Pruning} Agenten durch die große Anzahl der durchgeführten Züge ausgeglichen.
\item Die \mxZitat{Alpha-Beta Pruning} Suche besitzt eine zu geringe Suchtiefe. Durch eine größere Suchtiefe werden bessere Ergebnisse erreicht. Allerdings steigt die Berechnungszeit eines Zuges selbst bei der Erhöhung der Suchtiefe nur um eins sehr stark an. Dadurch kann der Agent nur bis zu einer bestimmten Suche (max. 5) eine \mxZitat{Alpha-Beta Pruning} Suche durchführen, bevor die Wartedauer auf den nächsten Zug den erwarteten Nutzen übersteigt. 
\end{itemize} 
\section{Bewertung der Heuristiken}
\label{sect:Fazit:Heuristiken}
Im nachfolgenden Abschnitt wird auf die jeweilige Leistung der einzelnen Heuristiken in den durchgeführten Vergleichen eingegangen. Insbesondere bei der \mxZitat{Stored Monte-Carlo Heuristik} werden dabei mögliche Verbesserungsmöglichkeiten besprochen.  
\subsection{Die Heuristik \mxZitat{Nijssen 07}}
Die Implementierung der von \cite{nijssen_2007} vorgeschlagenen Heuristik kann in den hier durchgeführten Vergleichen nicht überzeugen. Tritt der Agent der sie im Spiel gegen den \mxZitat{Random}-Agenten einsetzt als Spieler 1 auf, so werden bei ihrem Einsatz sogar weniger Spiele gewonnen als der zufällig spielende Agent gewinnt (Vgl. 2). Zwar spielt der Agent besser wenn er als Spieler 2 auftritt (Vgl. 7), jedoch legt das Ergebniss des Vergleiches des zufällig spielenden Agenten mit sich selbst (Vgl. 0) nahe, dass der als Spieler 2 auftretende Agent im wesentlichen eine um rund 10\% höhere Gewinnchance hat. Diese Marge deckt sich im wesentlichen mit der angesprochenen Verbesserung.
\\Damit kann die Heuristik, ohne weitere Verbesserungen, nicht für den Einsatz empfohlen werden. Von einem Einsatz bei der Untersuchung des Spielverhaltens des \mxZitat{Alpha-Beta Pruning}-Agenten gegen einen menschlichen Spieler ist abzusehen. 
\subsection{Die Heuristik \mxZitat{Stored Monte-Carlo}}
\label{fz:h_smc}
Nutzt der \mxZitat{Alpha-Beta Pruning}-Agent die \mxZitat{Stored Monte-Carlo} Heuristik so, gewinnt er in beiden Spielkombinationen häufiger als der \mxZitat{Random} Agent (siehe Abbildung \ref{tbl:cmp-results} Vergleiche 3 und 8). Im Vergleich mit den anderen Heuristiken ist die Gewinnwahrscheinlichkeit jedoch sehr gering.
\\Im Spiel \mxZitat{AB (Stored Monte Carlo Heuristik) } gegen \mxZitat{Random} (siehe Vergleich 3) gewinnt der Agent in 1000 Spielen nur vier Spiele mehr als der zufällige Spieler (458 zu 454). In den 1000 Spielen \mxZitat{Random} gegen \mxZitat{AB (Stored Monte Carlo Heuristik)} gewinnt die Heuristik mit 446 zu 550 gewonnenen Spielen (siehe Vergleich 8).
\\Aus den Vergleichen 3 und 8 lassen sich folgende Erkenntnisse ableiten:
\begin{itemize}
\item Der Agent \mxZitat{Alpha-Beta Pruning} gewinnt bei der Verwendung der \mxZitat{Stored Monte-Carlo} als zweiter Spieler deutlich mehr Spiele als als erster Spieler.
\item Wird der Agent als erster Spieler eingesetzt ergibt sich keine deutlich höhere Gewinnwahrscheinlichkeit als die des zufälligen Spielers. Dies bedeutet, dass der Agent sehr schlecht spielt, es also genauso gut wäre Züge zufällig auszuwählen. 
\item Auffällig sind die 88 Spiele, welche im Vergleich 3 unentschieden endeten. Im Gegensatz dazu gab es im Vergleich 8 nur vier unentschiedene Spiele. Die Gewinnwahrscheinlichkeit des zufälligen Spielers bleibt allerdings annähernd gleich bei 45,4\% bzw. 44,6\%. Man könnte dieses Ergebnis so interpretieren, dass der Agent in Vergleich die Spiele zwar nicht gewinnen konnte, aber immerhin verhindern konnte, dass diese Spiele verloren wurden.
\item Die Spiele im Vergleich 8 sind durchschnittlich 37,8 Sekunden schneller die Spiele im Vergleich 3.
\end{itemize}
Die Nutzung der Heuristik \mxZitat{Stored Monte-Carlo} wird aufgrund der o.g. Ergebnissen nicht empfohlen.
\\Es gibt mehrere mögliche Ursachen der schlechten Gewinnwahrscheinlichkeiten der Heuristik.
\vspace{0.5cm}
\\Die erste Ursache ist, dass die Datenbank, auf welcher die Heuristik basiert, nicht die Gewinnwahrscheinlichkeit des Spielers bei der Durchführung eines Zuges in der aktuellen Zugnummer im aktuellen Spiel liefert. Stattdessen gibt die Datenbank die Gewinnwahrscheinlichkeit des Spielers zurück, der in der aktuellen Zugnummer den Zug ausführt, zurück.
\\Der Unterschied zwischen den Aussagen besteht darin, dass die Datenbank den aktuellen Spielzustand (bereits durchgeführte Spielzüge) vernachlässigt und nur die statistische Wahrscheinlichkeit über alle Züge zurückgibt, in welchen in der aktuellen Zugnummer der Zug zu einem Gewinn geführt hat.
\\Dadurch können Spielsituationen auftreten, in welchen der Zug mit der, laut Datenbank, höchsten Gewinnwahrscheinlichkeit schlechter ist als ein Zug mit einer vermeintlich geringeren Gewinnwahrscheinlichkeit, da die jeweilige Spielsituation die Wahrscheinlichkeiten stark beeinflusst.
\vspace{0.5cm}
\\Es müsste auch evaluiert werden, ob die Zusammenfassung der Spielfelder in zehn Spielkategorien eine zu starke Vereinfachung des Spielfeldes darstellt. Das Spielfeld ist zwar symmetrisch aufgebaut, es könnten aber dennoch Seiteneffekte auftreten.
\vspace{0.5cm}
\\Dies führt zu einer weiteren möglichen Ursache der schlechten Heuristik. Aus den in der Datenbank gespeicherten Tripel aus gewonnen Spielen des ersten / zweiten Spielers und der Gesamtanzahl der durchgeführten Spiele wird nur die Gewinnwahrscheinlichkeit berechnet. Die Gesamtanzahl der durchgeführten Spiele wird allerdings nicht berücksichtigt. Es kann durchaus vorkommen, dass einzelne Feldkategorien unterschiedlich oft gespielt werden. So kann eine Feldkategorie sehr selten gespielt werden, dann aber eine relativ hohe Gewinnwahrscheinlichkeit besitzen, und eine Feldkategorie sehr oft mit einer geringeren Gewinnwahrscheinlichkeit gespielt werden. Die Heuristik bevorzugt in diesem Fall die selten gespielte Feldkategorie, da die Gewinnwahrscheinlichkeit höher ist. Da die Gesamtanzahl aller durchgeführten Spielzüge einer Zugnummer, sieht man von vorzeitig beendeten Spielen ab, konstant sind (rd. 140.000), ist es u.U. auch sinnvoll die Gesamtanzahl der Spiele einer Feldkategorie der verfügbaren Feldkategorien in die Berechnung der Heuristik einzubinden.

\subsection{Die Heuristik \mxZitat{Cowthello}}
Die \mxZitat{Cowthello} Heuristik bietet die höchsten Gewinnwahrscheinlichkeiten der \mxZitat{Alpha-Beta Pruning} Heuristiken. Folgende Ergebnisse können aus den Vergleichen 4 und 9 der Tabelle \ref{tbl:cmp-results} abgeleitet werden:
\begin{itemize}
\item In den 1000 Spielen des \mxZitat{Alpha-Beta Pruning} Agenten mit der \mxZitat{Cowthello} Heuristik gegen den \mxZitat{Random} Agenten gewinnt der \mxZitat{Alpha-Beta Pruning} Agent 564 zu 324 Spiele.
\item Die große Zahl von 112 unentschiedenen Spielen fällt bei dieser Heuristik ebenfalls auf.
\item In den 1000 Spielen des \mxZitat{Random} Agenten gegen den \mxZitat{Alpha-Beta Pruning} Agenten mit der \mxZitat{Cowthello} Heuristik gewinnt der \mxZitat{Alpha-Beta Pruning} Agent 709 zu 289 Spiele.
\item Die durchschnittliche Spielzeit des Agenten ist als zweiter Spieler ebenfalls geringer als die des Agenten als erster Spieler. Die Spielzeit der \mxZitat{Cowthello} Heuristik ist die kleinste aller verwendeten Heuristiken.
\end{itemize}
Damit ist die \mxZitat{Cowthello}-Heuristik die beste der betrachteten Heuristiken. Entsprechend ist ihr Einsatz für den Agenten \mxZitat{\abp} zu empfehlen.

%\section{Bewertung der Vorgehensweise}

\section{Ausblick}
\paragraph{Weitere Verfolgung des Ansatzes der \mxZitat{Stored Monte Carlo}-Heuristik}
Die in der \mxZitat{Stored Monte-Carlo} Heuristik verwendetet Datenbank könnte womöglich zur Erstellung eines verbesserten Agenten benutzt werden. Die Funktionsweise des neuen Agenten entspräche dabei im wesentlichen der des \mxZitat{Monte Carlo} Agenten. Der Unterschied bestünde darin, bei der Auswahl von Zügen in den simulierten Spielen einen zufälligen Zug so zu wählen, dass dieser jeweils mit einer der in der Datenbank für diesen Zug gespeicherten Wahrscheinlichkeit proportionalen Wahrscheinlichkeit ausgewählt wird. Werden die Ergebnisse der simulierten Spiele zusätzlich in der Datenbank gespeichert, so verbessert sich der Agenten mit jedem gespielten Spiel.
\\Dieser Prozess ist aber zwangsläufig mit Schreibzuggriffen oder mindestens das Verändern des Datenbankenobjektes verbunden. Dies hat zwei schwerwiegende Auswirkungen:
\begin{itemize}
\item Die Parallelisierung der gleichzeitigen Ausführung zufälliger Spiele ist schwieriger, da auf Dateiebene nur ein Thread gleichzeitig in die Datenbank schreiben kann.
\item Die zusätzliche Datenbankinteraktion benötigt mehr Zeit.
\end{itemize}
Das Resultat der beiden Auswirkungen ist, dass die Anzahl der durchgeführten zufälligen Spiele reduziert werden muss wenn die Berechnungszeit nicht wachsen soll.
\\Es ist zu evaluieren, ob der Mehrwert der Datenbanknutzung größer ist als der bisherige \mxZitat{Monte Carlo} Agent, welcher eine höhere Anzahl an zufällig durchgeführter Spiele besitzt.
\paragraph{Bedeutung für die Methode}

